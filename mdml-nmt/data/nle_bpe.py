"""
This file includes modifications to subword_nmt distributed through GitHub at https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/apply_bpe.py under this license https://github.com/rsennrich/subword-nmt/blob/master/LICENSE

Copyright with respect to the modifications: Copyright 2021 Naver Corporation

ORIGINAL COPYRIGHT NOTICE AND PERMISSION NOTICE:

Copyright (c) 2015 University of Edinburgh

MIT License. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"""

import os
import re
import regex
import json
import itertools
import unicodedata
import collections
from fairseq import utils
from fairseq.data.encoders import register_bpe


@register_bpe('nle_bpe')
class NLE_BPE(object):

    @staticmethod
    def add_args(parser):
        # fmt: off
        parser.add_argument('--bpe-codes', '--bpecodes', help='path to BPE codes generated by learn_bpe.py, relative to data dir')
        # fmt: on

    def __init__(self, args):
        bpe_path = None
        if getattr(args, 'bpe_codes', None):
            filenames = [args.bpe_codes]
        else:
            filenames = [f'bpecodes.{args.source_lang}', 'bpecodes']
        for filename in filenames:
            path = os.path.join(args.data, filename)
            if os.path.exists(path):
                bpe_path = path
                break
        
        assert bpe_path is not None, 'no bpecodes found'

        config = {}
        with open(bpe_path) as f:
            first_line = next(f)
            if first_line.startswith('#'):
                try:
                    config = json.loads(first_line.strip('# \n\r'))
                except:
                    pass
            else:
                f = itertools.chain([first_line], f)

            self.bpe_codes = [tuple(line.rstrip('\r\n').rsplit(' ', maxsplit=1)) for line in f]
            self.bpe_codes = {code: i for i, code in reversed(list(enumerate(self.bpe_codes)))}

        self.lowercase = config.get('lowercase', False)
        self.nfkc = config.get('nfkc', False)
        self.inline_case = config.get('inline_case', False)
        self.protect_regex = config.get('protect_regex')
        self.protect_regex = None if self.protect_regex is None else regex.compile(self.protect_regex)

        self.meta_symbol = '▁'
        self.protect_symbol = '╳'

        self.whitespace_regex = regex.compile(r'\s+')
        self.no_mixed_case_regex = regex.compile(
            '({0}?[[:upper:]]?[^[:upper:]\s{0}{1}]+|{0}?[[:upper:]]+|{0}|{1})'.format(
                regex.escape(self.meta_symbol),
                regex.escape(self.protect_symbol)
            )
        )
        self.sentencepiece_regex = regex.compile(
            '({0}?[^\s{0}{1}]+|{0}|{1})'.format(
                regex.escape(self.meta_symbol),
                regex.escape(self.protect_symbol)
            )
        )
        self.upper_code, self.title_code, self.lower_code = range(3)
        self.case_symbols = ['<U>', '<T>', None]
        self.cache = collections.OrderedDict()  # must be ordered to work as an LRU cache

    def encode(self, x: str, **kwargs) -> str:
        sentence = x.strip()

        if not sentence:
            return sentence

        if self.nfkc:
            sentence = unicodedata.normalize('NFKC', sentence)

        if self.protect_regex is not None:
            sentence = sentence.replace(self.protect_symbol, ' ')
            protected_tokens = [
                m.group(0) for m in self.protect_regex.finditer(sentence)
            ]
            sentence = self.protect_regex.sub(self.protect_symbol, sentence)

        if self.inline_case:
            for symbol in self.case_symbols:
                if symbol is not None:
                    sentence = sentence.replace(symbol, ' ')

        sentence = sentence.replace(self.meta_symbol, ' ')
        sentence = self.meta_symbol + self.whitespace_regex.sub(self.meta_symbol, sentence)
        if self.inline_case:
            tokens = self.no_mixed_case_regex.findall(sentence)
        else:
            tokens = self.sentencepiece_regex.findall(sentence)

        if self.lowercase or self.inline_case:
            cased_tokens = tokens
            tokens = [token.lower() for token in tokens]

        wordpieces = [[] if not word else self.encode_word_cached(word) for word in tokens]

        if self.inline_case:
            wordpieces_ = []
            for cased_token, wordpiece in zip(cased_tokens, wordpieces):
                i = 0
                wordpiece_ = []
                for out in wordpiece:
                    x = cased_token[i:i + len(out)]
                    i += len(out)
                    if x.isupper():
                        wordpiece_.append((out, self.upper_code))
                    elif x.istitle():
                        wordpiece_.append((out, self.title_code))
                    else:
                        wordpiece_.append((out, self.lower_code))
                wordpieces_.append(wordpiece_)

            wordpieces = [
                ' '.join(self.add_factor(token, case) for (token, case) in wordpiece)
                for wordpiece in wordpieces_
            ]
        else:
            wordpieces = [' '.join(wordpiece) for wordpiece in wordpieces]

        sentence = ' '.join(wordpieces)

        if self.protect_regex is not None:
            sentence = sentence.replace(self.protect_symbol + ' ▁ ', self.protect_symbol + ' ')
            for token in protected_tokens:
                sentence = sentence.replace(self.protect_symbol, token, 1)
            sentence = self.whitespace_regex.sub(' ', sentence)

        if sentence.startswith(self.meta_symbol + ' '):
            # a lone meta symbol at the beginning of a sentence serves no purpose
            sentence = sentence[len(self.meta_symbol + ' '):]

        return sentence

    def add_factor(self, token, case):
        if self.inline_case:
            case_symbol = self.case_symbols[case]
            if case_symbol is not None:
                token += ' ' + case_symbol
        return token

    def encode_word(self, word):
        word = list(word)

        while len(word) > 1:
            pairs = {pair: None for pair in zip(word, word[1:]) if pair in self.bpe_codes}
            # using dict instead of set, because set has a non-deterministic order

            if not pairs:
                break

            bigram = min(pairs, key=lambda pair: self.bpe_codes.get(pair, float('inf')))
            if bigram not in self.bpe_codes:
                break
            first, second = bigram

            new_word = []
            skip = False
            for x, y in zip(word, word[1:]):
                if skip:
                    skip = False
                    continue

                if x == first and y == second:
                    new_word.append(x + y)
                    skip = True
                else:
                    new_word.append(x)
            if not skip:
                new_word.append(y)

            word = new_word

        return word

    def encode_word_cached(self, word):
        # simple LRU cache implementation (functools.lru_cache is not pickable)
        if word in self.cache:
            new_word = self.cache.pop(word)
            self.cache[word] = new_word   # put this entry last in the cache
            return new_word
        else:
            new_word = self.encode_word(word)
            self.cache[word] = new_word
            if len(self.cache) > 2**20:
                word = next(iter(self.cache.keys()))   # delete first (oldest) entry
                self.cache.pop(word)
            return new_word

    def decode(self, x: str, **kwargs) -> str:
        tokens = x.split()
        for i, w in enumerate(tokens):
            if w == '<T>':
                tokens[i - 1] = tokens[i - 1].title()
            elif w == '<U>':
                tokens[i - 1] = tokens[i - 1].upper()
        x = ' '.join(w for w in tokens if w != '<T>' and w != '<U>')
        x = x.replace(' ', '').replace('▁', ' ').strip()
        return x
